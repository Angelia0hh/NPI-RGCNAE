import argparse
import configparser
import numpy as np
import torch
import torch.nn as nn
import scipy.sparse as sp
import torch.optim as optim
import torch.nn.functional as F
import pandas as pd
from model import *
from utils import *
import os

DEVICE = torch.device('cpu')
SCORES = torch.tensor([-1, 1]).to(DEVICE)

def load_dataset(dataset, filepath, identity_feature, negative_random_sample, identity_feature_dim=1024):
    if dataset == 'NPInter_10412':
        filepath = filepath + 'NPInter_10412/'
    elif dataset == 'RPI13254':
        filepath = filepath + 'RPI13254/'
    elif dataset == 'RPI1807':
        filepath = filepath + 'RPI1807/'
    elif dataset == 'RPI488':
        filepath = filepath + 'RPI488/'
    elif dataset == 'RPI369':
        filepath = filepath + 'RPI369/'
    elif dataset == 'RPI7317':
        filepath = filepath + 'RPI7317/'
    elif dataset == 'RPI1446':
        filepath = filepath + 'RPI1446/'
    elif dataset == 'RPI2241':
        filepath = filepath + 'RPI2241/'
    elif dataset == 'NPInter_4158':
        filepath = filepath + 'NPInter_4158/'
    elif dataset == 'RPI369':
        filepath = filepath + 'RPI369/'

    NPI_pos_matrix = pd.read_csv(filepath + 'NPI_pos.csv', header=None).values

    name = ['index']
    for i in range(1024):
        name.append(i + 1)

    NPI_neg_matrix = pd.read_csv(filepath + "NPI_neg_" + negative_random_sample + ".csv", header=None).values
    edgelist = pd.read_csv(filepath + 'edgelist_' + negative_random_sample + '.csv', header=None)
    # emb = pd.read_csv(filepath + 'emd_'+negative_random_sample+'.emd.txt', header=None, sep=' ', names=name)

    protein_side_feature = pd.read_csv(filepath + 'Protein3merfeat.csv').values
    RNA_side_feature = pd.read_csv(filepath + 'ncRNA4merfeat.csv').values
    supplement = np.zeros((RNA_side_feature.shape[0], 87))  # 通过补零补齐到同一维度
    RNA_side_feature = np.concatenate((RNA_side_feature, supplement), axis=1)

    if (identity_feature == 'one hot'):
        identity_feature = np.identity(NPI_pos_matrix.shape[0] + NPI_pos_matrix.shape[1], dtype=np.float32)
        RNA_identity_feature, protein_identity_feature = identity_feature[
                                                         :NPI_pos_matrix.shape[0]], identity_feature[
                                                                                    NPI_pos_matrix.shape[0]:]
    elif (identity_feature == 'random'):
        feature = np.random.randn(NPI_pos_matrix.shape[0] + NPI_pos_matrix.shape[1], identity_feature_dim)
        RNA_identity_feature, protein_identity_feature = feature[:NPI_pos_matrix.shape[0]], feature[
                                                                                            NPI_pos_matrix.shape[0]:]
    elif (identity_feature == 'kmer'):
        RNA_identity_feature = RNA_side_feature
        protein_identity_feature = protein_side_feature
    return NPI_pos_matrix, NPI_neg_matrix, protein_identity_feature, \
           RNA_identity_feature, protein_side_feature, RNA_side_feature, edgelist


def train(NPI_pos_matrix, NPI_neg_matrix, protein_identity_feature, RNA_identity_feature, protein_side_feature,
          RNA_side_feature, edgelist,
          NODE_INPUT_DIM, SIDE_FEATURE_DIM, GCN_HIDDEN_DIM, SIDE_HIDDEN_DIM, ENCODE_HIDDEN_DIM,
          threshold, ressavepath,use_side_feature, accumulate_strategy,
          EPOCHS, DROPOUT_RATIO, INITIAL_LEARNING_RATE, layers, WEIGHT_DACAY, step_size, gamma):
    '''

    :param NPI_pos_matrix: binary matrix of positive samples，shape:[ncRNA_num,protein_num]
    :param NPI_neg_matrix: binary matrix of negative samples， shape:[ncRNA_num, protein_num]
    :param protein_identity_feature: identity feature matirx of proteins, node2vec or one hot, shape:[protein_num, identity_feature_dim]
    :param RNA_identity_feature: identity feature matirx of RNA,node2vec or one hot ,shape:[RNA_num, identity_feature_dim]
    :param protein_side_feature: side_feature matirx of protein, 3mer, shape:[protein_num,343]
    :param RNA_side_feature: side_feature matirx of RNA, 4mer, shape:[RNA_num,256]
    :param edgelist: ncRNA-protein pairs, columns = [pair_index,RNA_index,protein_index,label]
    :param NODE_INPUT_DIM:identity_feature_dim
    :param SIDE_FEATURE_DIM:side_feature_dim
    :param GCN_HIDDEN_DIM:dimension of embeddings generated by GCN
    :param SIDE_HIDDEN_DIM: dimension of side_feature after fully connected layer transforming
    :param ENCODE_HIDDEN_DIM: the final node representation dimension after combining embeddings and side_feature
    :param threshold: >threshold regard as interaction, otherwise non-interaction
    :param probsavepath: path of saving prediction probability
    :param metricssavepath: path of saving result metrics
    :param use_side_feature:True or False
    :param accumulate_strategy:method of node aggregating (stack or sum)
    :param EPOCHS: training epochs
    :param DROPOUT_RATIO:
    :param INITIAL_LEARNING_RATE: the initial learning rate of training
    :param layers:the number of GCN layers
    :param WEIGHT_DACAY:
    :param step_size: after step_size steps, learning rate will descend
    :param gamma:the percentage of each learning rate droping
    :return:
    '''


    case_study_RNA = ['NEAT1','MALAT1','LINC00273']

    RPI7317 = pd.read_csv('data/raw_data/RPI7317.csv')
    protein_list = RPI7317['Protein names'].unique().tolist()
    ncRNA_list = RPI7317['RNA names'].unique().tolist()

    RNA_dicts = {}
    for RNA, protein in RPI7317['Protein names'].groupby(RPI7317['RNA names']):
        RNA_dicts[RNA] = list(protein)


    RNA_side_feature = tensor_from_numpy(RNA_side_feature, DEVICE).float()
    protein_side_feature = tensor_from_numpy(protein_side_feature, DEVICE).float()
    RNA_identity_feature = tensor_from_numpy(RNA_identity_feature, DEVICE).float()
    protein_identity_feature = tensor_from_numpy(protein_identity_feature, DEVICE).float()

    for case in case_study_RNA:
        # 找到所有已知和cast_study 关联的对
        pos_related_pairs = []  # 和case RNA所有相关的正样本 index对
        for pr in RNA_dicts[case]:
            pos_related_pairs.append([ncRNA_list.index(case), protein_list.index(pr)])

        print(len(pos_related_pairs))

        # 找到所有需要在训练集中去掉的RNA-蛋白质对的下标
        delete_pairs = pos_related_pairs  # 去掉2/3的相关的正样本
        delete_pairs = np.array(delete_pairs)

        # 将case_study中对的信息从训练集中去掉
        mask = np.ones((len(ncRNA_list), len(protein_list)))
        mask[delete_pairs[:, 0], delete_pairs[:, 1]] = 0  # 将验证集中的边置0
        print("raw dataset:")
        print(NPI_pos_matrix.sum())
        print(NPI_neg_matrix.sum())
        tmp_pos = NPI_pos_matrix * mask
        tmp_neg = NPI_neg_matrix * mask

        print("The number of negative samples in the matrix:")
        print(tmp_neg.sum())
        print("The number of positive samples in the matrix:")
        print(tmp_pos.sum())

        RNA2protein_adj = []
        RNA2protein_adj.append(tmp_neg)
        RNA2protein_adj.append(tmp_pos)
        protein2RNA_adj = []
        protein2RNA_adj.append(tmp_neg.T)
        protein2RNA_adj.append(tmp_pos.T)
        print("The number of negative samples in the matrix:")
        print(tmp_neg.sum())
        print("The number of positive samples in the matrix:")
        print(tmp_pos.sum())
        RNA2protein_adj = globally_normalize_bipartite_adjacency(RNA2protein_adj, False)
        protein2RNA_adj = globally_normalize_bipartite_adjacency(protein2RNA_adj, False)

        # 将numpy.array 转为Tensor
        RNA2protein_adj = [to_torch_sparse_tensor(adj, DEVICE) for adj in RNA2protein_adj]
        protein2RNA_adj = [to_torch_sparse_tensor(adj, DEVICE) for adj in protein2RNA_adj]

        rows = edgelist[(edgelist[1].isin(delete_pairs[:, 0])) & (edgelist[2].isin(delete_pairs[:, 1]))].index
        train_data = edgelist.drop(rows).values  # 将要预测的互作对从index列表中去掉
        RNA_indices = train_data[:, 1]
        protein_indices = train_data[:, 2]
        RNA_indices = tensor_from_numpy(RNA_indices, DEVICE).long()
        protein_indices = tensor_from_numpy(protein_indices, DEVICE).long()
        labels = tensor_from_numpy((train_data[:, 3]), DEVICE).long()

        # 构建测试集：RNA和所有未出现在训练集与RNA有关对中的protein
        test_data = [] # [RNA_index,protein_index,label]
        '''
        for i in range(len(protein_list)):
            if [ncRNA_list.index(case), i] not in pos_related_pairs[:len(pos_related_pairs) ]:
                    if [ncRNA_list.index(case), i] in pos_related_pairs[len(pos_related_pairs)//6:]:
                        test_data.append([ncRNA_list.index(case), i,1])
                    else:
                        test_data.append([ncRNA_list.index(case), i, 0])
        '''
        for i in range(len(protein_list)):
            if [ncRNA_list.index(case), i] in pos_related_pairs:
                test_data.append([ncRNA_list.index(case), i, 1])
            else:
                test_data.append([ncRNA_list.index(case), i, 0])
        test_data = np.array(test_data)

        model = GraphMatrixCompletion(NODE_INPUT_DIM, SIDE_FEATURE_DIM, GCN_HIDDEN_DIM,
                                      SIDE_HIDDEN_DIM, ENCODE_HIDDEN_DIM, use_side_feature=use_side_feature,
                                      accumulate_strategy=accumulate_strategy,
                                      dropout=DROPOUT_RATIO, num_basis=2, layers=layers).to(DEVICE)
        criterion = nn.CrossEntropyLoss().to(DEVICE)
        optimizer = optim.Adam(model.parameters(), lr=INITIAL_LEARNING_RATE, weight_decay=WEIGHT_DACAY)
        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma, last_epoch=-1)



        for e in range(30):
            model.train()
            optimizer.zero_grad()

            model_inputs = (RNA2protein_adj, protein2RNA_adj,
                            RNA_identity_feature, protein_identity_feature,
                            RNA_side_feature, protein_side_feature,
                            RNA_indices, protein_indices,)
            logits, _, _ = model(*model_inputs)
            prob = F.softmax(logits, dim=1).detach()
            pred_y = torch.sum(prob * SCORES, dim=1).detach()
            y = pred_y.clone().detach()
            y[y > threshold] = 1
            y[y <= threshold] = 0
            loss = criterion(logits, (labels + 1) // 2)

            loss.backward()  # 反向传播计算参数的梯度
            optimizer.step()  # 使用优化方法进行梯度更新
            scheduler.step()


            print("Epoch:{}".format(e))
            printN(y, (labels + 1) // 2)
            train_acc = accuracy(y, (labels + 1) // 2)
            train_pre = precision(y, (labels + 1) // 2)
            train_sen = sensitivity(y, (labels + 1) // 2)
            train_spe = specificity(y, (labels + 1) // 2)
            train_MCC = MCC(y, (labels + 1) // 2)
            train_FPR = FPR(y, (labels + 1) // 2)
            train_TPR = TPR(y, (labels + 1) // 2)
            print("accuracy:{}, PRE:{}, SEN:{}, SPE:{}, MCC:{}, FPR:{}, TPR:{}".
                  format(train_acc, train_pre, train_sen, train_spe, train_MCC, train_FPR, train_TPR))
            print('\n')

        #  测试
        model.eval()
        with torch.no_grad():
            RNA_test = test_data[:,0]
            protein_test = test_data[:,1]
            RNA_test = tensor_from_numpy(RNA_test, DEVICE).long()
            protein_test = tensor_from_numpy(protein_test, DEVICE).long()
            model_inputs = (RNA2protein_adj, protein2RNA_adj,
                            RNA_identity_feature, protein_identity_feature,
                            RNA_side_feature, protein_side_feature,
                            RNA_test, protein_test,)
            test_logits, embedding_protein, embedding_RNA = model(*model_inputs)
            test_labels = tensor_from_numpy(test_data[:,2], DEVICE).long()
            test_prob = F.softmax(test_logits, dim=1).detach()
            test_pred_y = torch.sum(test_prob * SCORES, dim=1).detach()

            print("==========================")
            print('validation')
            # print("probability:")
            # print(test_prob)
            # print("prediction:")
            test_y = test_pred_y.clone().detach()
            test_y[test_y > threshold] = 1
            test_y[test_y <= threshold] = 0
            # print(test_y[test_mask])
            # print("score:")
            # print(test_pred_y[test_mask])
            # print("label:")
            # print(test_labels)
            printN(test_y, (test_labels + 1) // 2)
            print(len(test_data[:,2]))
            print(len(test_pred_y))
            test_AUC = AUC(test_data[:, 2].squeeze(), test_pred_y.detach().numpy().squeeze())
            test_acc = accuracy(test_y, (test_labels + 1) // 2)
            test_pre = precision(test_y, (test_labels + 1) // 2)
            test_sen = sensitivity(test_y, (test_labels + 1) // 2)
            test_spe = specificity(test_y, (test_labels + 1) // 2)
            test_MCC = MCC(test_y, (test_labels + 1) // 2)
            test_FPR = FPR(test_y, (test_labels + 1) // 2)
            test_TPR = TPR(test_y, (test_labels + 1) // 2)

            #保存每个case的预测结果
            result = []
            for i,res in enumerate(test_pred_y.detach().numpy()):
                result.append([test_data[i,0],test_data[i,1],case,protein_list[test_data[i,1]],res,test_data[i,2]])
            result = pd.DataFrame(result,columns=['RNA_index','protein_index','RNA_name','protein_name','score','label'])
            result.to_csv(ressavepath+case+".csv",index=False)


if __name__ == "__main__":

    filepath = 'data/generated_data/'
    savepath = 'result/'

    INI_PATH = 'dataset_settings.ini'
    parser = argparse.ArgumentParser(
        description="""R-GCN Graph Autoencoder for NcRNA-protein Link Prediciton """)

    parser.add_argument('-dataset',
                        type=str, help='choose a dataset to implement 5-fold cross validation', default='RPI7317')

    args = parser.parse_args()
    DATA_SET = args.dataset

    config = configparser.ConfigParser()
    config.read(INI_PATH)

    INITIAL_LEARNING_RATE = config.getfloat(DATA_SET, 'INITIAL_LEARNING_RATE')
    WEIGHT_DACAY = config.getfloat(DATA_SET, 'WEIGHT_DACAY')
    DROPOUT_RATIO = config.getfloat(DATA_SET, 'DROPOUT_RATIO')
    step_size = config.getint(DATA_SET, 'step_size')
    gamma = config.getfloat(DATA_SET, 'gamma')
    layers = config.getint(DATA_SET, 'layers')
    EPOCHS = config.getint(DATA_SET, 'EPOCHS')
    SIDE_FEATURE_DIM = config.getint(DATA_SET, 'SIDE_FEATURE_DIM')
    GCN_HIDDEN_DIM = config.getint(DATA_SET, 'GCN_HIDDEN_DIM')
    SIDE_HIDDEN_DIM = config.getint(DATA_SET, 'SIDE_HIDDEN_DIM')
    ENCODE_HIDDEN_DIM = config.getint(DATA_SET, 'ENCODE_HIDDEN_DIM')


    # 读入原始的数据和特征
    NPI_pos_matrix, NPI_neg_matrix, protein_identity_feature, RNA_identity_feature, \
    protein_side_feature, RNA_side_feature, edgelist = \
        load_dataset(dataset=DATA_SET, filepath=filepath, identity_feature_dim=1024,
                     identity_feature='random', negative_random_sample='sort')

    train(NPI_pos_matrix, NPI_neg_matrix, protein_identity_feature, RNA_identity_feature,
          protein_side_feature,
          RNA_side_feature, edgelist, NODE_INPUT_DIM=1024, SIDE_FEATURE_DIM=SIDE_FEATURE_DIM,
          GCN_HIDDEN_DIM=GCN_HIDDEN_DIM,
          SIDE_HIDDEN_DIM=SIDE_HIDDEN_DIM, ENCODE_HIDDEN_DIM=ENCODE_HIDDEN_DIM,
          use_side_feature=True, accumulate_strategy='stack',
          threshold=0, INITIAL_LEARNING_RATE=INITIAL_LEARNING_RATE, WEIGHT_DACAY=WEIGHT_DACAY,
          DROPOUT_RATIO=DROPOUT_RATIO, step_size=step_size, layers=layers, EPOCHS=EPOCHS,
          gamma=gamma,
          ressavepath=savepath+DATA_SET+'/case_study_')




